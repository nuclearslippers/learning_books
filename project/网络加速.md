# 针对NVIDIA ORIN NX的模型推理加速技巧

## 电源模式
```bash
sudo nvpmodel -q   # 查看当前模式
sudo nvpmodel -m 0 # 高性能模式
sudo nvpmodel -m 1 # 平衡模式
sudo nvpmodel -m 2 # 节能模式
sudo systemctl restart nvpower
```

## tensorRT加速
```bash
# 安装
sudo apt-get install -y python3-libnvinfer libnvinfer-dev libnvinfer-plugin-dev tensorrt
# .pt 转 .onnx
## 一般来说，YOLO自带动导出onnx的功能
# .onnx 转 .engine
/usr/src/tensorrt/bin/trtexec --onnx=<onnx_file> --saveEngine=<engine_file>
```